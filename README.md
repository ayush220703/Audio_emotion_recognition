# ðŸŽ¤ Speech Emotion Recognition

This project classifies human emotions from speech using deep learning. It uses the RAVDESS dataset and a hybrid CNN-BiLSTM-Attention model to identify 8 emotions from both speech and song audio. The model achieves over **82% accuracy**, with most classes exceeding **75% precision**.

---

## ðŸ“ Project Structure


---

## ðŸ“Œ Dataset: RAVDESS

- ðŸŽ§ [RAVDESS Dataset on Zenodo](https://zenodo.org/record/1188976)
- Full name: *Ryerson Audio-Visual Database of Emotional Speech and Song*
- 24 actors expressing 8 emotions through speech and song
- File format: `Modality-Voice-Emotion-Intensity-Statement-Repetition-Actor.wav`
- Audio: Downsampled from 48 kHz to 22.05 kHz

---

## ðŸ˜ƒ Emotion Labels

| Label ID | Emotion   |
|----------|-----------|
| 1        | Neutral   |
| 2        | Calm      |
| 3        | Happy     |
| 4        | Sad       |
| 5        | Angry     |
| 6        | Fear      |
| 7        | Disgust   |
| 8        | Surprise  |

---

## âš™ï¸ Preprocessing Pipeline

1. **Load Audio**  
   - All `.wav` files from both modalities are loaded.

2. **Label Extraction**  
   - Emotion is extracted from filenames, mapped to 0-based index.

3. **Log-Mel Spectrograms**  
   - Parameters:  
     - Duration: 3 sec  
     - Sampling: 22.05 kHz  
     - `n_mels`: 128 â†’ Output shape: (128, 128)

4. **Train-Test Split**  
   - Stratified 80/20 split

5. **Normalization**  
   - Train stats used to normalize both sets:  
     ```
     X_normalized = (X - mean_train) / std_train
     ```

6. **Data Augmentation (Train Only)**  
   - Apply either Gaussian noise or random time masking (10%)

7. **Reshaping**  
   - Spectrograms reshaped to (samples, 128, 128, 1)

8. **Label Encoding**  
   - One-hot encoded to 8-dimensional vectors

9. **Class Weights**  
   - Calculated with `sklearn`; surprise class boosted Ã—1.5

---

## ðŸ§  Model Architecture

> Hybrid CNN + BiLSTM + Attention


- **Loss**: Categorical Crossentropy (with label smoothing)  
- **Optimizer**: Adam  
- **Attention**: Custom temporal attention layer

---

## ðŸ“Š Results

| Metric         | Score     |
|----------------|-----------|
| Accuracy       | 82%       |
| Macro F1       | 83%       |
| Weighted F1    | 82%       |

âœ… All emotion classes achieve **â‰¥75% precision**

---

## ðŸš€ Getting Started

### 1. Clone this Repo

```bash
git clone https://github.com/your-username/Speech-Emotion-Recognition.git
cd Speech-Emotion-Recognition
pip install numpy librosa tensorflow
python test_model.py sample_audio/test_audio.wav

---
t me know if youâ€™d like help generating a `requirements.txt`, a license file, or a demo badge!
